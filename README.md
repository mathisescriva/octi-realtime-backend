# OKTI Realtime Backend

Backend Node.js/TypeScript pour l'agent vocal intelligent OKTI, utilisant l'API OpenAI Realtime (GA) pour les JournÃ©es Portes Ouvertes de l'ESCE.

## ğŸ“‹ Description

OKTI est un assistant vocal en temps rÃ©el conÃ§u pour rÃ©pondre aux questions des Ã©tudiants et prospects lors des JournÃ©es Portes Ouvertes de l'ESCE. Le systÃ¨me permet une interaction speech-to-speech fluide avec une latence minimale, grÃ¢ce Ã  l'API OpenAI Realtime.

### FonctionnalitÃ©s principales

- **Communication vocale en temps rÃ©el** : Interaction speech-to-speech via WebSocket
- **Recherche documentaire intelligente (RAG)** : AccÃ¨s Ã  une base de connaissances enrichie (brochures, guides Ã©tudiants, conventions de stages, profils LinkedIn)
- **PersonnalitÃ© dynamique** : Assistant enjouÃ© et orientÃ© international, adaptÃ© au public Ã©tudiant
- **Gestion robuste des erreurs** : Gestion automatique des rate limits et rÃ©initialisation de session
- **Architecture modulaire** : Code structurÃ© et rÃ©utilisable pour d'autres agents

## ğŸ—ï¸ Architecture

```
src/
â”œâ”€â”€ server.ts                 # Point d'entrÃ©e principal
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ index.ts              # Configuration Express
â”‚   â”œâ”€â”€ httpRoutes/           # Routes HTTP
â”‚   â”‚   â”œâ”€â”€ healthRoute.ts
â”‚   â”‚   â”œâ”€â”€ sessionRoute.ts   # CrÃ©ation de sessions Ã©phÃ©mÃ¨res
â”‚   â”‚   â””â”€â”€ ragRoute.ts       # Endpoint de recherche RAG
â”‚   â””â”€â”€ wsHandlers/
â”‚       â””â”€â”€ realtimeHandler.ts # Handler WebSocket principal
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ realtime/             # Client OpenAI Realtime
â”‚   â”‚   â”œâ”€â”€ OpenAIRealtimeClient.ts
â”‚   â”‚   â””â”€â”€ types.ts
â”‚   â”œâ”€â”€ agents/               # Configuration des agents
â”‚   â”‚   â”œâ”€â”€ AgentConfig.ts
â”‚   â”‚   â”œâ”€â”€ octiAgent.ts
â”‚   â”‚   â””â”€â”€ esceContext.ts    # Contexte complet ESCE
â”‚   â”œâ”€â”€ sessions/             # Gestion des sessions
â”‚   â”‚   â””â”€â”€ SessionManager.ts
â”‚   â””â”€â”€ tools/                 # Outils et fonctions
â”‚       â””â”€â”€ ragSearchTool.ts  # Recherche RAG (Pinecone + OpenAI)
â”œâ”€â”€ config/                   # Configuration
â”‚   â”œâ”€â”€ env.ts
â”‚   â””â”€â”€ logger.ts
â””â”€â”€ utils/                     # Utilitaires
    â”œâ”€â”€ wsMessages.ts
    â””â”€â”€ errors.ts

scripts/
â””â”€â”€ ingest.ts                 # Script d'ingestion des documents dans Pinecone

documents/                     # Documents source pour RAG
â”œâ”€â”€ brochures/
â”œâ”€â”€ guides/
â”œâ”€â”€ stages/
â””â”€â”€ linkedin/
```

## ğŸš€ Installation

### PrÃ©requis

- Node.js â‰¥ 20.0.0
- npm ou yarn
- ClÃ© API OpenAI
- (Optionnel) ClÃ© API Pinecone pour la fonctionnalitÃ© RAG

### Installation des dÃ©pendances

```bash
npm install
```

### Configuration

1. Copier le fichier d'exemple de configuration :

```bash
cp .env.example .env
```

2. Ã‰diter le fichier `.env` avec vos variables d'environnement :

```env
# Configuration serveur
PORT=8080
NODE_ENV=production

# OpenAI
OPENAI_API_KEY=sk-xxx
OPENAI_REALTIME_MODEL=gpt-realtime

# Configuration agent OKTI
OKTI_SYSTEM_PROMPT="Tu es OKTI..."
OKTI_DEFAULT_VOICE=verse
OKTI_PROMPT_ID=pmpt_xxx  # Optionnel : utiliser un prompt ID

# RAG (Optionnel)
PINECONE_API_KEY=xxx
PINECONE_INDEX_NAME=esce-documents
```

## ğŸš€ Guide de dÃ©marrage rapide

### 1. DÃ©marrer le backend

Dans le rÃ©pertoire racine du projet :

```bash
npm run dev
```

Le serveur backend dÃ©marre sur `http://localhost:8080` avec rechargement automatique.

**VÃ©rification :** Ouvrez `http://localhost:8080/health` dans votre navigateur. Vous devriez voir :
```json
{
  "status": "ok",
  "timestamp": "...",
  "service": "octi-realtime-backend"
}
```

### 2. DÃ©marrer le frontend de dÃ©mo (optionnel)

Pour tester OKTI avec l'interface de dÃ©mo Next.js :

```bash
cd reference-agents
npm install  # Si ce n'est pas dÃ©jÃ  fait
npm run dev
```

Le frontend dÃ©marre gÃ©nÃ©ralement sur `http://localhost:3000` ou `http://localhost:3001` (selon les ports disponibles).

**AccÃ¨s Ã  la dÃ©mo :**
- Ouvrez `http://localhost:3000` (ou le port indiquÃ© dans la console)
- SÃ©lectionnez le scÃ©nario **"octi"** dans le menu dÃ©roulant
- Cliquez sur **"Connect"** pour dÃ©marrer la session
- Autorisez l'accÃ¨s au microphone si demandÃ©
- Parlez avec OKTI !

### 3. Utilisation en production

```bash
npm run build
npm start
```

## ğŸ¯ Utilisation

### DÃ©veloppement

```bash
npm run dev
```

Le serveur dÃ©marre sur `http://localhost:8080` avec rechargement automatique.

### Production

```bash
npm run build
npm start
```

### Ingestion des documents (RAG)

Pour ingÃ©rer les documents dans Pinecone :

```bash
npm run ingest
```

Cette commande :
- Parse les PDFs, fichiers Excel et autres documents dans `documents/`
- CrÃ©e des embeddings via OpenAI
- Stocke les vecteurs dans Pinecone

## ğŸ“¡ API

### WebSocket : `/ws/realtime`

Endpoint principal pour la conversation vocale en temps rÃ©el.

**URL :** `ws://localhost:8080/ws/realtime`

**Protocole :** Voir [API.md](./API.md) pour la documentation complÃ¨te du protocole WebSocket.

### HTTP : `/api/session`

CrÃ©e une session Ã©phÃ©mÃ¨re OpenAI Realtime pour connexion WebRTC directe.

**MÃ©thode :** `GET`

**RÃ©ponse :**
```json
{
  "id": "sess_xxx",
  "client_secret": {
    "value": "sk-xxx",
    "expires_at": 1234567890
  }
}
```

### HTTP : `/api/rag/search`

Effectue une recherche dans la base de connaissances RAG.

**MÃ©thode :** `POST`

**Body :**
```json
{
  "query": "stages en finance"
}
```

**RÃ©ponse :**
```json
{
  "context": "Contexte pertinent extrait des documents..."
}
```

### Health Check : `/health`

VÃ©rifie que le serveur est opÃ©rationnel.

**MÃ©thode :** `GET`

**RÃ©ponse :**
```json
{
  "status": "ok",
  "timestamp": "2024-01-15T10:30:00Z"
}
```

## ğŸ”§ Configuration avancÃ©e

### Personnalisation de l'agent

La personnalitÃ© et le contexte d'OKTI sont dÃ©finis dans :
- `src/core/agents/esceContext.ts` : Contexte complet sur l'ESCE
- `reference-agents/src/app/agentConfigs/octiAgent.ts` : Configuration de l'agent (frontend WebRTC)

### Configuration RAG

Pour activer la recherche documentaire :

1. CrÃ©er un index Pinecone (dimensions: 1536, metric: cosine)
2. Configurer `PINECONE_API_KEY` et `PINECONE_INDEX_NAME` dans `.env`
3. Placer les documents dans `documents/` (brochures, guides, stages, linkedin)
4. ExÃ©cuter `npm run ingest`

### Gestion des rate limits

Le systÃ¨me gÃ¨re automatiquement les erreurs de rate limit OpenAI :
- DÃ©tection automatique des erreurs `rate_limit_exceeded`
- Extraction du temps d'attente depuis le message d'erreur
- RÃ©initialisation automatique de la session aprÃ¨s le dÃ©lai
- Messages d'erreur clairs pour l'utilisateur

### Reconnexion automatique

Le systÃ¨me inclut une gestion robuste des dÃ©connexions :
- **Reconnexion automatique** : Jusqu'Ã  5 tentatives avec backoff exponentiel
- **Surveillance continue** : VÃ©rification de l'Ã©tat de la connexion toutes les 5 secondes
- **DÃ©tection proactive** : Reconnexion automatique en cas de perte de connexion
- **Messages informatifs** : L'utilisateur est informÃ© des tentatives de reconnexion

## ğŸ“š Documentation

- **[API.md](./API.md)** : Documentation complÃ¨te de l'API (WebSocket et HTTP)
- **[RAG_SETUP.md](./RAG_SETUP.md)** : Guide de configuration RAG
- **[RAG_ARCHITECTURE.md](./RAG_ARCHITECTURE.md)** : Architecture dÃ©taillÃ©e du systÃ¨me RAG

## ğŸ› ï¸ Technologies

- **Node.js** â‰¥ 20.0.0
- **TypeScript** 5.3+
- **Express** : Serveur HTTP
- **ws** : WebSocket server
- **OpenAI Realtime API** : Communication vocale en temps rÃ©el
- **Pinecone** : Base de donnÃ©es vectorielle (RAG)
- **Pino** : Logging structurÃ©

## ğŸ“¦ Scripts disponibles

```bash
npm run build          # Compilation TypeScript
npm run start          # DÃ©marrage en production
npm run dev            # DÃ©marrage en dÃ©veloppement (watch mode)
npm run type-check     # VÃ©rification des types TypeScript
npm run ingest         # Ingestion des documents dans Pinecone
```

## ğŸ”’ SÃ©curitÃ©

- Les clÃ©s API ne doivent jamais Ãªtre commitÃ©es dans le repository
- Le fichier `.env` est ignorÃ© par Git (voir `.gitignore`)
- Utilisation de variables d'environnement pour toutes les configurations sensibles
- Validation des entrÃ©es utilisateur sur tous les endpoints

## ğŸ› DÃ©pannage

### Le serveur ne dÃ©marre pas

- VÃ©rifier que `OPENAI_API_KEY` est dÃ©fini dans `.env`
- VÃ©rifier que le port 8080 n'est pas dÃ©jÃ  utilisÃ©
- Consulter les logs pour plus de dÃ©tails

### Erreurs de rate limit

- Le systÃ¨me gÃ¨re automatiquement les rate limits
- En cas de limite frÃ©quente, considÃ©rer :
  - Optimiser la taille du contexte (rÃ©duire `esceContext.ts`)
  - Augmenter le quota OpenAI
  - Limiter le nombre de sessions simultanÃ©es

### ProblÃ¨mes de recherche RAG

- VÃ©rifier que Pinecone est configurÃ© (`PINECONE_API_KEY`)
- VÃ©rifier que l'index existe et contient des donnÃ©es (`npm run ingest`)
- Consulter les logs pour les erreurs de recherche

### Le chatbot s'arrÃªte pendant la conversation

Le systÃ¨me inclut une reconnexion automatique, mais si le problÃ¨me persiste :
- VÃ©rifier les logs du backend pour identifier l'erreur
- VÃ©rifier que la connexion WebSocket n'est pas bloquÃ©e par un firewall
- VÃ©rifier que `OPENAI_API_KEY` est valide et n'a pas expirÃ©
- Le systÃ¨me tente automatiquement de se reconnecter jusqu'Ã  5 fois

## ğŸ“ Licence

MIT

## ğŸ‘¥ Support

Pour toute question ou problÃ¨me, consulter la documentation dans `API.md` ou contacter l'Ã©quipe de dÃ©veloppement.

---

**DÃ©veloppÃ© pour les JournÃ©es Portes Ouvertes de l'ESCE**
