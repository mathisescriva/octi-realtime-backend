# OCTI Realtime Backend

Backend Node.js/TypeScript pour l'agent IA vocal OCTI utilisant l'API OpenAI Realtime (GA).

## üéØ Objectif

Backend simple, fiable et r√©utilisable qui fait le proxy entre le frontend et l'API OpenAI Realtime pour permettre une communication speech-to-speech en temps r√©el avec une latence minimale.

**Conforme √† la documentation OpenAI Realtime API GA** - Utilise les derni√®res sp√©cifications de l'API.

## ‚ú® Fonctionnalit√©s

- ‚úÖ **WebSocket Proxy** : Proxy bidirectionnel entre frontend et OpenAI Realtime API
- ‚úÖ **Sessions √âph√©m√®res** : Route `/api/session` pour cr√©er des sessions √©ph√©m√®res (WebRTC)
- ‚úÖ **Audio Streaming** : Support PCM16 avec Base64 encoding via `input_audio_buffer.append`
- ‚úÖ **Voice Activity Detection** : Utilise `semantic_vad` pour d√©tection automatique de la parole
- ‚úÖ **Multi-Agent Ready** : Architecture extensible pour plusieurs agents
- ‚úÖ **Production Ready** : Pr√™t pour d√©ploiement sur Render, Railway, etc.

## üöÄ D√©marrage rapide

### Pr√©requis

- Node.js ‚â• 20
- npm ou yarn
- Cl√© API OpenAI avec acc√®s √† l'API Realtime

### Installation

```bash
# Cloner le repository
git clone <your-repo-url>
cd octi-realtime-backend

# Installer les d√©pendances
npm install

# Copier le fichier d'environnement
cp .env.example .env

# √âditer .env et remplir vos variables
# Notamment OPENAI_API_KEY et OCTI_SYSTEM_PROMPT
```

### Configuration (.env)

```env
PORT=8080
NODE_ENV=development
OPENAI_API_KEY=sk-xxx
OPENAI_REALTIME_MODEL=gpt-realtime
OCTI_SYSTEM_PROMPT="Tu es OCTI, l'assistant vocal intelligent..."
OCTI_DEFAULT_VOICE=alloy
OCTI_PROMPT_ID=pmpt_xxx  # Optionnel : utiliser un prompt ID au lieu de instructions
```

### Lancer en d√©veloppement

```bash
npm run dev
```

Le serveur d√©marre sur `http://localhost:8080`.

### Build et production

```bash
# Compiler TypeScript
npm run build

# Lancer le serveur
npm start
```

## üì° API Endpoints

### GET /health

V√©rifie que le serveur est op√©rationnel.

**R√©ponse :**
```json
{
  "status": "ok",
  "timestamp": "2024-01-01T12:00:00.000Z",
  "service": "octi-realtime-backend"
}
```

### GET /api/session

Cr√©e une session √©ph√©m√®re OpenAI Realtime. Utilis√© par les frontends WebRTC (comme le repo de r√©f√©rence OpenAI).

**R√©ponse :**
```json
{
  "object": "realtime.session",
  "id": "sess_xxx",
  "model": "gpt-realtime",
  "client_secret": {
    "value": "ek_xxx",
    "expires_at": 1234567890
  },
  ...
}
```

### POST /api/client-secret

G√©n√®re une cl√© √©ph√©m√®re pour connexion directe √† OpenAI (alternative √† `/api/session`).

### WebSocket /ws/realtime

Endpoint WebSocket pour conversation directe. Voir [Protocole WebSocket](#-protocole-websocket) ci-dessous.

## üì° Protocole WebSocket

### Endpoint

```
wss://<BACKEND_DOMAIN>/ws/realtime
```

### Messages Frontend ‚Üí Backend

#### 1. D√©marrer la conversation

```json
{ "type": "start_conversation" }
```

#### 2. Envoyer un chunk audio (binaire)

Envoy√© en `ArrayBuffer` (PCM16, 24kHz), pas de JSON. Le backend convertit automatiquement en Base64 et l'envoie via `input_audio_buffer.append`.

```javascript
ws.send(pcm16Buffer);
```

#### 3. Fin de la parole utilisateur

```json
{ "type": "user_audio_end" }
```

**Note :** Avec `semantic_vad` activ√©, ce message n'est g√©n√©ralement pas n√©cessaire car OpenAI d√©tecte automatiquement la fin de parole.

#### 4. Reset session

```json
{ "type": "reset_session" }
```

### Messages Backend ‚Üí Frontend

#### 1. Backend pr√™t

```json
{ "type": "ready" }
```

Envoy√© automatiquement lorsque la session Realtime est initialis√©e et confirm√©e par OpenAI.

#### 2. Chunk audio du mod√®le (binaire)

Audio PCM16 (24kHz) √† jouer directement. Re√ßu en `ArrayBuffer`.

#### 3. Fin de la r√©ponse vocale

```json
{ "type": "bot_audio_end" }
```

#### 4. Transcription texte (optionnel, pour affichage)

```json
{ "type": "transcript_delta", "text": "..." }
```

#### 5. Erreur

```json
{ "type": "error", "message": "..." }
```

## üèóÔ∏è Architecture

```
src/
  server.ts                 # Point d'entr√©e du serveur
  app/
    index.ts                # Configuration Express
    httpRoutes/
      healthRoute.ts        # Route GET /health
      sessionRoute.ts      # Route GET /api/session (sessions √©ph√©m√®res)
      clientSecretRoute.ts # Route POST /api/client-secret
    wsHandlers/
      realtimeHandler.ts    # Handler WebSocket principal
  core/
    realtime/
      OpenAIRealtimeClient.ts  # Client WebSocket OpenAI
      types.ts                 # Types pour l'API Realtime (GA)
    agents/
      AgentConfig.ts          # Configuration g√©n√©rique d'agent
      octiAgent.ts            # Configuration sp√©cifique OCTI
    sessions/
      SessionManager.ts       # Gestionnaire de sessions
  config/
    env.ts                    # Configuration environnement
    logger.ts                 # Logger Pino
  utils/
    wsMessages.ts             # Types et helpers messages WS
    errors.ts                 # Erreurs personnalis√©es
```

## üìù Exemple d'utilisation (Frontend WebSocket)

```javascript
const ws = new WebSocket('wss://your-backend.com/ws/realtime');

ws.onopen = () => {
  console.log('Connexion √©tablie');
};

ws.onmessage = (event) => {
  // Message JSON
  if (typeof event.data === 'string') {
    const message = JSON.parse(event.data);
    
    switch (message.type) {
      case 'ready':
        console.log('Backend pr√™t');
        break;
      case 'bot_audio_end':
        console.log('R√©ponse audio termin√©e');
        break;
      case 'transcript_delta':
        console.log('Transcription:', message.text);
        break;
      case 'error':
        console.error('Erreur:', message.message);
        break;
    }
  } 
  // Audio binaire (PCM16, 24kHz)
  else {
    const audioBuffer = event.data;
    // Jouer l'audio
    playAudio(audioBuffer);
  }
};

// D√©marrer la conversation
ws.send(JSON.stringify({ type: 'start_conversation' }));

// Envoyer un chunk audio (PCM16, 24kHz)
ws.send(audioChunk);

// Signaler la fin de l'audio utilisateur (optionnel avec VAD)
ws.send(JSON.stringify({ type: 'user_audio_end' }));
```

## üåê Utilisation avec un Frontend Personnalis√©

### Option 1 : WebSocket Direct (comme `voice-agent.html`)

Votre frontend se connecte directement au WebSocket `/ws/realtime` et envoie/re√ßoit de l'audio PCM16.

**Avantages :**
- Contr√¥le total sur l'audio
- Simple √† impl√©menter
- Pas de d√©pendances externes

**Exemple :** Voir `examples/voice-agent.html`

### Option 2 : WebRTC via Sessions √âph√©m√®res (comme le repo de r√©f√©rence OpenAI)

Votre frontend utilise `/api/session` pour obtenir une cl√© √©ph√©m√®re et se connecte directement √† OpenAI via WebRTC.

**Avantages :**
- Meilleure latence (connexion directe)
- Gestion automatique de l'audio par WebRTC
- Support des interruptions et guardrails

**Exemple :** Voir le repo `reference-agents/` (non inclus dans ce repo)

## üö¢ D√©ploiement

### Sur Render

1. Cr√©er un nouveau **Web Service** sur Render
2. Connecter votre repository GitHub
3. Configurer :
   - **Build Command** : `npm install && npm run build`
   - **Start Command** : `npm start`
   - **Environment Variables** : Ajouter toutes les variables de `.env.example`
4. D√©ployer

Le service sera accessible sur `https://your-service.onrender.com`

### Sur Railway / Heroku / Autres

M√™me principe : configurer les variables d'environnement et utiliser `npm start` comme commande de d√©marrage.

### Variables d'environnement requises

- `OPENAI_API_KEY` : **Requis** - Votre cl√© API OpenAI
- `OCTI_SYSTEM_PROMPT` ou `OCTI_PROMPT_ID` : **Requis** - Instructions ou ID de prompt
- `PORT` : Port d'√©coute (d√©faut: 8080)
- `OPENAI_REALTIME_MODEL` : Mod√®le √† utiliser (d√©faut: `gpt-realtime`)
- `OCTI_DEFAULT_VOICE` : Voix √† utiliser (d√©faut: `alloy`)

## üß™ Tests

### V√©rifier que le serveur r√©pond

```bash
curl http://localhost:8080/health
```

### Tester la route /api/session

```bash
curl http://localhost:8080/api/session
```

### Tester avec le frontend d'exemple

```bash
# D√©marrer un serveur HTTP simple
python3 -m http.server 8000

# Ouvrir http://localhost:8000/examples/voice-agent.html
```

## üì¶ D√©pendances

- **express** : Serveur HTTP
- **ws** : WebSocket
- **dotenv** : Variables d'environnement
- **pino** : Logger performant
- **typescript** : Compilation TypeScript

## üîí S√©curit√©

- ‚úÖ Ne jamais commiter le fichier `.env`
- ‚úÖ Utiliser des variables d'environnement pour les secrets
- ‚úÖ Valider tous les messages WebSocket entrants
- ‚úÖ G√©rer proprement les erreurs et fermer les connexions
- ‚úÖ CORS configur√© pour permettre les requ√™tes frontend

## üéØ Conformit√© avec OpenAI Realtime API GA

Ce backend est conforme √† la documentation officielle OpenAI Realtime API (GA) :

- ‚úÖ Structure `session.update` conforme
- ‚úÖ Utilisation de `input_audio_buffer.append` avec Base64
- ‚úÖ Support de `semantic_vad` pour la d√©tection de parole
- ‚úÖ Support des prompts par ID (`prompt.id`)
- ‚úÖ Format audio PCM16 √† 24kHz
- ‚úÖ Gestion correcte des √©v√©nements GA (`response.output_audio.delta`, etc.)

## üìÑ Licence

MIT

## ü§ù Contribution

Ce projet est une baseline propre et fonctionnelle. Pour ajouter de nouveaux agents ou fonctionnalit√©s :

1. Cr√©er un nouveau fichier dans `src/core/agents/`
2. Ajouter la configuration dans `src/core/sessions/SessionManager.ts`
3. Suivre l'architecture existante
