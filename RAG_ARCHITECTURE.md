# üß† Architecture RAG pour OKTI (Ingestion de Documents)

## üìã Documents √† Ing√©rer

1. **Brochures ESCE** (PDFs)
2. **Guides d'√©tudiants** (PDFs)
3. **Historiques de stage avec √©tudiants en poste** (Excel)
4. **Profils LinkedIn d'√©tudiants pass√©s** (PDFs)

## üéØ Contraintes

- **Latence minimale** : La recherche ne doit pas ralentir la conversation
- **Temps r√©el** : R√©ponses vocales fluides
- **Pr√©cision** : Contexte pertinent pour chaque question

---

## üèóÔ∏è Architecture Recommand√©e : RAG avec Pr√©-processing

### Principe

**Pr√©-processing (hors ligne) :**
- Ing√©rer les documents une seule fois
- Cr√©er des embeddings vectoriels
- Stocker dans une base vectorielle

**Pendant la conversation (temps r√©el) :**
- Recherche s√©mantique rapide (< 100ms)
- Injection du contexte via Tools
- R√©ponse vocale imm√©diate

---

## üì¶ Stack Technique Recommand√©e

### Option 1 : OpenAI Embeddings + Qdrant (Recommand√©)

**Avantages :**
- ‚úÖ Tr√®s rapide (< 50ms pour recherche)
- ‚úÖ Gratuit (Qdrant open-source, peut tourner en local)
- ‚úÖ Facile √† d√©ployer
- ‚úÖ Bonne int√©gration avec OpenAI

**Composants :**
- `text-embedding-3-small` (OpenAI) - Rapide et pas cher
- Qdrant (Vector DB) - Open-source, performant
- PDF parsing : `pdf-parse` ou `pdfjs-dist`
- Excel parsing : `xlsx` ou `exceljs`

### Option 2 : OpenAI Embeddings + Pinecone

**Avantages :**
- ‚úÖ Managed service (pas de maintenance)
- ‚úÖ Tr√®s rapide
- ‚úÖ Scalable

**Inconv√©nients :**
- ‚ùå Co√ªt mensuel (gratuit jusqu'√† 1M vectors)
- ‚ùå D√©pendance externe

### Option 3 : OpenAI Embeddings + PostgreSQL + pgvector

**Avantages :**
- ‚úÖ Utilise votre DB existante
- ‚úÖ Pas de service externe

**Inconv√©nients :**
- ‚ùå Plus lent que Qdrant/Pinecone
- ‚ùå Configuration plus complexe

---

## üîß Impl√©mentation D√©taill√©e

### Phase 1 : Ingestion (Script s√©par√©, ex√©cut√© une fois)

```typescript
// scripts/ingest-documents.ts
import { QdrantClient } from '@qdrant/js-client-rest';
import { OpenAI } from 'openai';
import pdfParse from 'pdf-parse';
import * as XLSX from 'xlsx';
import fs from 'fs';

const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });
const qdrant = new QdrantClient({ url: 'http://localhost:6333' });

// 1. Parser les documents
async function parsePDF(filePath: string): Promise<string> {
  const dataBuffer = fs.readFileSync(filePath);
  const data = await pdfParse(dataBuffer);
  return data.text;
}

async function parseExcel(filePath: string): Promise<string[]> {
  const workbook = XLSX.readFile(filePath);
  const sheets = workbook.SheetNames;
  const texts: string[] = [];
  
  for (const sheetName of sheets) {
    const sheet = workbook.Sheets[sheetName];
    const jsonData = XLSX.utils.sheet_to_json(sheet);
    texts.push(JSON.stringify(jsonData));
  }
  
  return texts;
}

// 2. Chunker les textes (chunks de ~500 tokens)
function chunkText(text: string, chunkSize: number = 500): string[] {
  const words = text.split(' ');
  const chunks: string[] = [];
  
  for (let i = 0; i < words.length; i += chunkSize) {
    chunks.push(words.slice(i, i + chunkSize).join(' '));
  }
  
  return chunks;
}

// 3. Cr√©er des embeddings
async function createEmbeddings(texts: string[]): Promise<number[][]> {
  const response = await openai.embeddings.create({
    model: 'text-embedding-3-small',
    input: texts,
  });
  
  return response.data.map(item => item.embedding);
}

// 4. Stocker dans Qdrant
async function storeInQdrant(
  collectionName: string,
  chunks: string[],
  embeddings: number[][],
  metadata: any[]
) {
  // Cr√©er la collection si elle n'existe pas
  await qdrant.createCollection(collectionName, {
    vectors: {
      size: 1536, // Dimension de text-embedding-3-small
      distance: 'Cosine',
    },
  });
  
  // Ins√©rer les points
  const points = chunks.map((chunk, index) => ({
    id: index,
    vector: embeddings[index],
    payload: {
      text: chunk,
      ...metadata[index],
    },
  }));
  
  await qdrant.upsert(collectionName, {
    wait: true,
    points,
  });
}

// 5. Pipeline complet
async function ingestDocuments() {
  const collectionName = 'esce_documents';
  
  // Ing√©rer les brochures
  const brochureFiles = fs.readdirSync('./documents/brochures');
  for (const file of brochureFiles) {
    const text = await parsePDF(`./documents/brochures/${file}`);
    const chunks = chunkText(text);
    const embeddings = await createEmbeddings(chunks);
    const metadata = chunks.map((_, i) => ({
      source: 'brochure',
      filename: file,
      chunkIndex: i,
    }));
    await storeInQdrant(collectionName, chunks, embeddings, metadata);
  }
  
  // Ing√©rer les guides √©tudiants
  const guideFiles = fs.readdirSync('./documents/guides');
  for (const file of guideFiles) {
    const text = await parsePDF(`./documents/guides/${file}`);
    const chunks = chunkText(text);
    const embeddings = await createEmbeddings(chunks);
    const metadata = chunks.map((_, i) => ({
      source: 'guide',
      filename: file,
      chunkIndex: i,
    }));
    await storeInQdrant(collectionName, chunks, embeddings, metadata);
  }
  
  // Ing√©rer les historiques de stage (Excel)
  const excelFiles = fs.readdirSync('./documents/stages');
  for (const file of excelFiles) {
    const texts = await parseExcel(`./documents/stages/${file}`);
    for (const text of texts) {
      const chunks = chunkText(text);
      const embeddings = await createEmbeddings(chunks);
      const metadata = chunks.map((_, i) => ({
        source: 'stage',
        filename: file,
        chunkIndex: i,
      }));
      await storeInQdrant(collectionName, chunks, embeddings, metadata);
    }
  }
  
  // Ing√©rer les profils LinkedIn (PDFs)
  const linkedinFiles = fs.readdirSync('./documents/linkedin');
  for (const file of linkedinFiles) {
    const text = await parsePDF(`./documents/linkedin/${file}`);
    const chunks = chunkText(text);
    const embeddings = await createEmbeddings(chunks);
    const metadata = chunks.map((_, i) => ({
      source: 'linkedin',
      filename: file,
      chunkIndex: i,
    }));
    await storeInQdrant(collectionName, chunks, embeddings, metadata);
  }
  
  console.log('‚úÖ Ingestion termin√©e !');
}

ingestDocuments();
```

### Phase 2 : Tool de Recherche (Backend)

```typescript
// src/core/tools/ragSearchTool.ts
import { QdrantClient } from '@qdrant/js-client-rest';
import { OpenAI } from 'openai';

const qdrant = new QdrantClient({ 
  url: process.env.QDRANT_URL || 'http://localhost:6333' 
});
const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });

export async function searchDocuments(query: string, limit: number = 3): Promise<string> {
  // 1. Cr√©er embedding de la requ√™te
  const queryEmbedding = await openai.embeddings.create({
    model: 'text-embedding-3-small',
    input: query,
  });
  
  // 2. Recherche vectorielle dans Qdrant
  const results = await qdrant.search('esce_documents', {
    vector: queryEmbedding.data[0].embedding,
    limit,
    score_threshold: 0.7, // Seuil de pertinence
  });
  
  // 3. Combiner les r√©sultats
  const context = results
    .map(result => result.payload?.text as string)
    .filter(Boolean)
    .join('\n\n');
  
  return context;
}

// Tool definition pour OpenAI Realtime API
export const ragSearchTool = {
  type: 'function',
  name: 'search_esce_documents',
  description: 'Recherche dans les brochures, guides √©tudiants, historiques de stage et profils LinkedIn de l\'ESCE. Utilise cette fonction quand un √©tudiant pose une question sur les programmes, les stages, les parcours d\'anciens √©tudiants, ou les informations g√©n√©rales de l\'√©cole.',
  parameters: {
    type: 'object',
    properties: {
      query: {
        type: 'string',
        description: 'La question ou le sujet de recherche (ex: "programme International Business", "stages en finance", "√©tudiants en marketing")',
      },
    },
    required: ['query'],
  },
};
```

### Phase 3 : Int√©gration dans Realtime Handler

```typescript
// src/app/wsHandlers/realtimeHandler.ts (modifications)

import { searchDocuments, ragSearchTool } from '../../core/tools/ragSearchTool';

// Dans SessionManager.createOctiSession, ajouter les tools :
const sessionConfig: RealtimeSessionConfig = {
  // ... config existante
  tools: [ragSearchTool], // Ajouter le tool
};

// Handler pour les tool calls
function handleOpenAIEvent(event: RealtimeEvent) {
  // ... handlers existants
  
  if (event.type === 'response.audio_transcript.delta') {
    // Log pour analytics
  }
  
  // Nouveau : G√©rer les tool calls
  if (event.type === 'response.audio_transcript.done') {
    // Si le mod√®le veut appeler un tool, il le fera via response.create avec tool_choice
  }
  
  if (event.type === 'conversation.item.input_audio_transcript.done') {
    // Le mod√®le peut d√©cider d'appeler un tool ici
  }
}
```

---

## ‚ö° Optimisations pour Latence

### 1. **Cache des Requ√™tes Fr√©quentes**

```typescript
// src/core/tools/ragSearchTool.ts
import NodeCache from 'node-cache';

const cache = new NodeCache({ stdTTL: 3600 }); // Cache 1h

export async function searchDocuments(query: string, limit: number = 3): Promise<string> {
  // V√©rifier le cache
  const cacheKey = `search:${query}:${limit}`;
  const cached = cache.get<string>(cacheKey);
  if (cached) {
    return cached; // < 1ms
  }
  
  // Recherche normale
  const context = await performSearch(query, limit);
  
  // Mettre en cache
  cache.set(cacheKey, context);
  
  return context;
}
```

### 2. **Pr√©-chargement des Embeddings**

Cr√©er les embeddings des requ√™tes fr√©quentes en amont.

### 3. **Limite de R√©sultats**

Limiter √† 3-5 r√©sultats les plus pertinents (pas besoin de tout).

### 4. **Qdrant en Local**

D√©ployer Qdrant sur le m√™me serveur que le backend pour latence minimale.

---

## üìä Estimation de Latence

| √âtape | Latence |
|-------|---------|
| Recherche dans cache | < 1ms |
| Cr√©ation embedding requ√™te | 50-100ms |
| Recherche vectorielle Qdrant | 10-30ms |
| **Total (sans cache)** | **60-130ms** |
| **Total (avec cache)** | **< 1ms** |

**Conclusion :** Avec cache, la latence est n√©gligeable. Sans cache, ~100ms est acceptable pour une recherche contextuelle.

---

## üöÄ Plan d'Impl√©mentation

### √âtape 1 : Setup Infrastructure
1. Installer Qdrant (Docker ou service managed)
2. Cr√©er dossier `documents/` avec sous-dossiers
3. Installer d√©pendances (`@qdrant/js-client-rest`, `pdf-parse`, `xlsx`)

### √âtape 2 : Script d'Ingestion
1. Cr√©er `scripts/ingest-documents.ts`
2. Parser PDFs et Excel
3. Cr√©er embeddings et stocker dans Qdrant

### √âtape 3 : Tool de Recherche
1. Cr√©er `src/core/tools/ragSearchTool.ts`
2. Impl√©menter `searchDocuments()`
3. D√©finir le tool pour OpenAI

### √âtape 4 : Int√©gration Backend
1. Ajouter `tools` dans `SessionManager`
2. G√©rer les tool calls dans `realtimeHandler`
3. Injecter le contexte dans la conversation

### √âtape 5 : Optimisations
1. Ajouter cache
2. Monitoring des performances
3. Ajuster les param√®tres (seuil, limite)

---

## üì¶ D√©pendances √† Ajouter

```json
{
  "dependencies": {
    "@qdrant/js-client-rest": "^1.7.0",
    "pdf-parse": "^1.1.1",
    "xlsx": "^0.18.5",
    "node-cache": "^5.1.2"
  },
  "devDependencies": {
    "@types/pdf-parse": "^1.1.4"
  }
}
```

---

## üîç Alternatives Simples (Si pas de Vector DB)

### Option : Recherche Full-Text Simple

Si vous voulez commencer simple sans Vector DB :

```typescript
// Recherche full-text dans les documents pr√©-pars√©s
// Stocker les textes dans un fichier JSON
// Recherche avec regex ou string matching
// Plus simple mais moins pr√©cis
```

**Recommandation :** Commencer avec Qdrant (gratuit, rapide, facile).


